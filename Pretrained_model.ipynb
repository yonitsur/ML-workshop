{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THiettcrmVIr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import yaml\n",
        "import sys\n",
        "import os\n",
        "from IPython import display\n",
        "import time\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUhdFqXbmWmG",
        "outputId": "e858c27c-2bc9-432d-e379-78dc742c0490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNEntoQxmY40",
        "outputId": "50b91896-1d9d-4f4e-820d-4f856e2723a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "Torchvision version: 0.15.2+cu118\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-_fhv626h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-_fhv626h\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36589 sha256=496635105cf066e88b7db8f27033a0175a6999f932c9fd2e810b59ccd10ff909\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-06p7rcno/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "from segment_anything.utils.amg import *\n",
        "from pycocotools import mask as mask_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bptX5swlmbut"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/MyDrive/Project/Project/data/Arik\"\n",
        "# folder_path = \"/content/drive/MyDrive/Project/data/Arik\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EibFt3ADmen9"
      },
      "outputs": [],
      "source": [
        "images = ['DJI_20230221091035_0034_V', 'DJI_20230221095213_0011_V', 'DJI_20230221100215_0124_V',\n",
        "          'DJI_20230221081744_0017_V']\n",
        "split = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOtyrh7Kmzpp"
      },
      "outputs": [],
      "source": [
        "labels_dict = {'Metal (general)':0, 'Metal (iron bender)':0, 'Metal (pipe)':0,\n",
        "               'Wood (pallett)':1, 'Wood (scraps/cuttings)':1,\n",
        "               'Carton':2,\n",
        "               'Concrete':3,\n",
        "               'Plastic (big bag)':4, 'Plastic (bucket)':4, 'Plastic (general)':4,\n",
        "               'Plastic (pipe)':4, 'Plastic (sand bag)':4,\n",
        "               'Nylon':5,\n",
        "               'Rubber':6,\n",
        "               'Textil/Fabric/Cloth':7,\n",
        "               'Background':8}\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(f'{folder_path}/{images[0]}/{images[0]}.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "uiJs0phPymA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = curr_dataset[0][0]\n",
        "# print(img)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Find the GLCM\n",
        "import skimage.feature as feature\n",
        "\n",
        "# Param:\n",
        "# source image\n",
        "# List of pixel pair distance offsets - here 1 in each direction\n",
        "# List of pixel pair angles in radians\n",
        "graycom = feature.greycomatrix(gray, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256)\n",
        "\n",
        "# Find the GLCM properties\n",
        "contrast = feature.greycoprops(graycom, 'contrast')\n",
        "dissimilarity = feature.greycoprops(graycom, 'dissimilarity')\n",
        "homogeneity = feature.greycoprops(graycom, 'homogeneity')\n",
        "energy = feature.greycoprops(graycom, 'energy')\n",
        "correlation = feature.greycoprops(graycom, 'correlation')\n",
        "ASM = feature.greycoprops(graycom, 'ASM')\n",
        "\n",
        "print(\"Contrast: {}\".format(contrast))\n",
        "print(\"Dissimilarity: {}\".format(dissimilarity))\n",
        "print(\"Homogeneity: {}\".format(homogeneity))\n",
        "print(\"Energy: {}\".format(energy))\n",
        "print(\"Correlation: {}\".format(correlation))\n",
        "print(\"ASM: {}\".format(ASM))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQKB2b2Gw9OT",
        "outputId": "deefe793-da59-4406-ef2f-e7049ecdf116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:35: skimage_deprecation: Function ``greycomatrix`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycomatrix`` instead.\n",
            "  removed_version='1.0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrast: [[275.08295767 478.6236908  302.40155111 480.78223043]]\n",
            "Dissimilarity: [[10.53462816 14.21255683 11.08607923 14.14114552]]\n",
            "Homogeneity: [[0.14030093 0.11010559 0.13737736 0.11163107]]\n",
            "Energy: [[0.01115822 0.00962345 0.01094506 0.0096626 ]]\n",
            "Correlation: [[0.95726568 0.92564564 0.95302494 0.92531037]]\n",
            "ASM: [[1.24505984e-04 9.26107083e-05 1.19794309e-04 9.33658192e-05]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:42: skimage_deprecation: Function ``greycoprops`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycoprops`` instead.\n",
            "  removed_version='1.0')\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:42: skimage_deprecation: Function ``greycoprops`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycoprops`` instead.\n",
            "  removed_version='1.0')\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:42: skimage_deprecation: Function ``greycoprops`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycoprops`` instead.\n",
            "  removed_version='1.0')\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:42: skimage_deprecation: Function ``greycoprops`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycoprops`` instead.\n",
            "  removed_version='1.0')\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:42: skimage_deprecation: Function ``greycoprops`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycoprops`` instead.\n",
            "  removed_version='1.0')\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/feature/__init__.py:42: skimage_deprecation: Function ``greycoprops`` is deprecated and will be removed in version 1.0. Use ``skimage.feature.graycoprops`` instead.\n",
            "  removed_version='1.0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLL_aIJTnDXR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "class SplitDatasetMaskedResized(Dataset):\n",
        "  def __init__(self, folder_path, image_name, sp):\n",
        "    self.image_name = image_name\n",
        "    self.sp = sp\n",
        "    self.im_path = f'{folder_path}/{image_name}'\n",
        "    self.split_path = f'{self.im_path}/split_{sp}'\n",
        "    with open(f'{self.split_path}/seg_{sp}.yaml') as f:\n",
        "      self.seg = yaml.safe_load(f)\n",
        "    files_in_folder = os.listdir(self.split_path)\n",
        "    self.ann_dict={image_name:{sp:{}}}\n",
        "    if f'anns_{sp}.yaml' in files_in_folder:\n",
        "      with open(f'{self.split_path}/anns_{sp}.yaml', 'r') as f:\n",
        "        self.ann_dict = yaml.safe_load(f)\n",
        "        # Remove all unknowns\n",
        "        self.ann_dict[self.image_name][self.sp] = {key:val for key, val in self.ann_dict[self.image_name][self.sp].items() if val != 'Unknown'}\n",
        "        # TODO: eliminate masks that are too small\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ann_dict[self.image_name][self.sp])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    annotated_masks = list(self.ann_dict[self.image_name][self.sp].keys())\n",
        "    m = annotated_masks[idx]\n",
        "    img = cv2.imread(f'{self.split_path}/{m}.jpg')\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    im_pil = Image.fromarray(img)\n",
        "    preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    input_tensor = preprocess(im_pil)\n",
        "\n",
        "    # mask = rle_to_mask(self.seg[self.image_name][self.sp][m]['segmentation'])\n",
        "    label = self.ann_dict[self.image_name][self.sp][m]\n",
        "    y = labels_dict[label]\n",
        "    return input_tensor, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqN7D1MOnFv1"
      },
      "outputs": [],
      "source": [
        "arr = []\n",
        "for image in images:\n",
        "  for split in range(16):\n",
        "    ds = SplitDatasetMaskedResized(folder_path, image, split)\n",
        "    arr.append(ds)\n",
        "\n",
        "curr_dataset = torch.utils.data.ConcatDataset(arr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = len(curr_dataset)\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds, test_ds = torch.utils.data.random_split(curr_dataset, [0.6, 0.2, 0.2], generator=generator1)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "valid_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "luJot_Oh-YnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sGQ-9sbm1ID",
        "outputId": "26fec2f9-3bd1-4dd5-9fce-8c704f3959e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([72.1773,  4.5802,  5.5521,  2.2507,  8.7947, 22.2675,  0.0000, 55.0827,\n",
            "         1.1893], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "lab = torch.zeros(9).cuda()\n",
        "for y in [x[1] for x in curr_dataset]:\n",
        "    lab[y] += 1\n",
        "lab = lab/lab.norm()\n",
        "lab = 1/lab\n",
        "lab[6] = 0\n",
        "print(lab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reOknhsLdvFY"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = {}\n",
        "dataset_sizes['train'] = len(train_ds)\n",
        "dataset_sizes['val'] = len(val_ds)\n",
        "dataset_sizes['test'] = len(test_ds)\n",
        "\n",
        "dataloaders = {}\n",
        "dataloaders['train'] = train_dataloader\n",
        "dataloaders['val'] = valid_dataloader\n",
        "dataloaders['test'] = test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSifHy21wY55"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Best available weights (currently alias for IMAGENET1K_V2)\n",
        "# Note that these weights may change across versions\n",
        "original_model = resnet50(weights=ResNet50_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsLFlmIqqs_4"
      },
      "outputs": [],
      "source": [
        "weights = ResNet50_Weights.DEFAULT\n",
        "# category_name = weights.meta['categories'][label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0g-JIB156Za"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "\n",
        "# extractor = AutoFeatureExtractor.from_pretrained(\"yangy50/garbage-classification\")\n",
        "\n",
        "# model = AutoModelForImageClassification.from_pretrained(\"yangy50/garbage-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPzeGMa_sb-E"
      },
      "outputs": [],
      "source": [
        "reverse_labels_dict = {0:'Metal',\n",
        "                1:'Wood',\n",
        "               2:'Carton',\n",
        "               3:'Concrete',\n",
        "               4:'Plastic',\n",
        "               5:'Nylon',\n",
        "               6:'Rubber',\n",
        "               7:'Textil/Fabric/Cloth',\n",
        "               8:'Background'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnPmyHjOrRuC"
      },
      "outputs": [],
      "source": [
        "running_vloss = 0.0\n",
        "running_vaccuracy = 0.0\n",
        "original_model.eval()\n",
        "# confusion_matrix = torch.zeros(9, 9)\n",
        "# Disable gradient computation and reduce memory consumption.\n",
        "with torch.no_grad():\n",
        "    for i, vdata in enumerate(test_dataloader):\n",
        "        vinputs, vlabels = vdata\n",
        "        vlabels = vlabels.cuda()\n",
        "        # vinputs = vinputs.permute(0, 3, 1, 2)\n",
        "        # print(vinputs.shape)\n",
        "        voutputs = original_model(vinputs).cuda()\n",
        "        # voutputs = model(vinputs)\n",
        "        # vloss = loss_fn(voutputs, vlabels)\n",
        "        pred = voutputs.argmax(dim=1)\n",
        "        for i in range(pred.shape[0]):\n",
        "          category_name = weights.meta['categories'][pred[i]]\n",
        "          real = reverse_labels_dict[vlabels[i].item()]\n",
        "          print(category_name + \" vs \" + real)\n",
        "        # running_vaccuracy += (vlabels == pred).float().mean()\n",
        "        # for t, p in zip(vlabels.view(-1), pred.view(-1)):\n",
        "        #     confusion_matrix[t.long(), p.long()] += 1\n",
        "        # per_class_running_accuracy += (vlabels == pred).float()\n",
        "        # running_vloss += vloss\n",
        "\n",
        "# avg_vloss = running_vloss / (i + 1)\n",
        "avg_vaccuracy = running_vaccuracy / (i + 1)\n",
        "\n",
        "# print('LOSS valid {}'.format(avg_vloss))\n",
        "print('ACCURACY train valid {}'.format(avg_vaccuracy))\n",
        "# print('Per class accuracy:',confusion_matrix.diag()/confusion_matrix.sum(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGOXctgbb1F8"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, test=False):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            if phase == 'val':\n",
        "                phase = 'test'\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if phase == 'test':\n",
        "                break\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuxq5VKifbBk"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "# print(classes)\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "# print(out)\n",
        "imshow(out, title=[reverse_labels_dict[x.item()] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKxfVkTJcu-C"
      },
      "source": [
        "Transfer Learning: Freezing all layers except final fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwW09_n9ctvI"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "# model_conv = torchvision.models.resnet18(weights='DEFAULT')\n",
        "for param in original_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = original_model.fc.in_features\n",
        "original_model.fc = nn.Linear(num_ftrs, 9)\n",
        "\n",
        "model = original_model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=lab) #reweight the loss such that each class has equal contribution to the loss\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 15 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=15, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIwHlD35dTXq"
      },
      "outputs": [],
      "source": [
        "model_transfered = train_model(model, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k53tZF-6vndj"
      },
      "source": [
        "Calc Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1tf8r0SvQoc"
      },
      "outputs": [],
      "source": [
        "train_model(model, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=1, test=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                # plt.figure(figsize=(5,5))\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(reverse_labels_dict[preds[j].item()]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "metadata": {
        "id": "bUh3j7JhhigP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GrshBgVdUAQ"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_transfered, num_images=6)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders['val']"
      ],
      "metadata": {
        "id": "HaYDfl0HDAbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(inputs, labels) in enumerate(dataloaders['val']):\n",
        "  print(i)\n",
        "  print(inputs, labels)"
      ],
      "metadata": {
        "id": "a1i--fUxDeIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders['val']"
      ],
      "metadata": {
        "id": "squK_CKaEO4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataloaders['val']:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "0EwdpEv5D8Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i,(inputs, labels) in enumerate(dataloaders['val']):\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ELdVBAgtikwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\n",
        "              'Metal',\n",
        "              'Wood',\n",
        "              'Carton',\n",
        "              'Concrete',\n",
        "              'Plastic',\n",
        "              'Nylon',\n",
        "              'Rubber',\n",
        "              'Textil/Fabric/Cloth',\n",
        "              'Background']\n",
        "\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  true_label, img = true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img,cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'green'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}%\\n({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  true_label = true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(len(predictions_array)), class_names, rotation=90)\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(len(predictions_array)), predictions_array, color=\"grey\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('green')"
      ],
      "metadata": {
        "id": "dHfDM2Lhj6ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(outputs.cpu().data)[4]"
      ],
      "metadata": {
        "id": "_zsr0vB645lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(labels.cpu().data)"
      ],
      "metadata": {
        "id": "WsAD3QqB5b6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(inputs.cpu().data)[4]"
      ],
      "metadata": {
        "id": "5j5XEE0H5C49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, np.array(outputs.cpu().data)[i], np.array(labels.cpu().data), np.array(inputs.cpu().data)[i])\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, np.array(outputs.cpu().data)[i],  np.array(labels.cpu().data))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YvwIjNiDlNKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, np.array(outputs.cpu().data)[i], np.array(labels.cpu().data), np.array(inputs.cpu().data)[i])\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, np.array(outputs.cpu().data)[i],  np.array(labels.cpu().data))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hAwaASxg2htn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 2\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, np.array(outputs.cpu().data)[i], np.array(labels.cpu().data), np.array(inputs.cpu().data)[i])\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, np.array(outputs.cpu().data)[i],  np.array(labels.cpu().data))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G-F0ZSLP2TTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 1\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "plt.subplots_adjust(wspace=0.1, hspace=2.5)\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, np.array(outputs.cpu().data)[i], np.array(labels.cpu().data), np.array(inputs.cpu().data)[i])\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, np.array(outputs.cpu().data)[i],  np.array(labels.cpu().data))\n",
        "# plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iIOeRcpkv3Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0sEstebuAxg"
      },
      "outputs": [],
      "source": [
        "torch.save(model_transfered.state_dict(), \"transfered_basic_resnet50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPwiUNzBwJSt"
      },
      "source": [
        "Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AjEEVXQwMFb"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "\n",
        "# extractor = AutoFeatureExtractor.from_pretrained(\"yangy50/garbage-classification\")\n",
        "\n",
        "# model = AutoModelForImageClassification.from_pretrained(\"yangy50/garbage-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQmTa7yoxzLg"
      },
      "outputs": [],
      "source": [
        "! pip install transformers pytorch-lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvzhwAuHxrrJ"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "# import pytorch_lightning as pl\n",
        "from torchmetrics import Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNZsImt52U4m"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "class SplitDatasetMaskedResizedVIT(Dataset):\n",
        "  def __init__(self, folder_path, image_name, sp):\n",
        "    self.image_name = image_name\n",
        "    self.sp = sp\n",
        "    self.im_path = f'{folder_path}/{image_name}'\n",
        "    self.split_path = f'{self.im_path}/split_{sp}'\n",
        "    with open(f'{self.split_path}/seg_{sp}.yaml') as f:\n",
        "      self.seg = yaml.safe_load(f)\n",
        "    files_in_folder = os.listdir(self.split_path)\n",
        "    self.ann_dict={image_name:{sp:{}}}\n",
        "    if f'anns_{sp}.yaml' in files_in_folder:\n",
        "      with open(f'{self.split_path}/anns_{sp}.yaml', 'r') as f:\n",
        "        self.ann_dict = yaml.safe_load(f)\n",
        "        # Remove all unknowns\n",
        "        self.ann_dict[self.image_name][self.sp] = {key:val for key, val in self.ann_dict[self.image_name][self.sp].items() if val != 'Unknown'}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ann_dict[self.image_name][self.sp])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    annotated_masks = list(self.ann_dict[self.image_name][self.sp].keys())\n",
        "    m = annotated_masks[idx]\n",
        "    img = cv2.imread(f'{self.split_path}/{m}.jpg')\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    im_pil = Image.fromarray(img)\n",
        "    preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    input_tensor = preprocess(im_pil)\n",
        "    # mask = rle_to_mask(self.seg[self.image_name][self.sp][m]['segmentation'])\n",
        "    label = self.ann_dict[self.image_name][self.sp][m]\n",
        "    y = labels_dict[label]\n",
        "    return input_tensor, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wIyluYc9FBC"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QeB-TQ49Ui5"
      },
      "outputs": [],
      "source": [
        "# ds=ImageFolder(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLHnrxjo2-FQ"
      },
      "outputs": [],
      "source": [
        "arr = []\n",
        "for image in images:\n",
        "  for split in range(16):\n",
        "    ds = SplitDatasetMaskedResizedVIT(folder_path, image, split)\n",
        "    # print(ds[0])\n",
        "    arr.append(ds)\n",
        "\n",
        "vit_curr_dataset = torch.utils.data.ConcatDataset(arr)\n",
        "\n",
        "vit_train_ds, vit_val_ds, vit_test_ds = torch.utils.data.random_split(vit_curr_dataset, [0.6, 0.2, 0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CexCPb9GgSMC"
      },
      "outputs": [],
      "source": [
        "lab = torch.zeros(9).cuda()\n",
        "for y in [x[1] for x in vit_curr_dataset]:\n",
        "    lab[y] += 1\n",
        "lab = lab/lab.norm()\n",
        "lab = 1/lab\n",
        "lab[6] = 0\n",
        "print(lab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8fZVijWymse"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationCollator:\n",
        "    def __init__(self, feature_extractor):\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        encodings = self.feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
        "        encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
        "        return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpx2XsjpyrzU"
      },
      "outputs": [],
      "source": [
        "feature_extractor = ViTFeatureExtractor(do_normalize=False).from_pretrained('google/vit-base-patch16-224-in21k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi2X7yYmyzhW"
      },
      "outputs": [],
      "source": [
        "vit_model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=9,\n",
        "    label2id={i:x for x, i in reverse_labels_dict.items()},\n",
        "    id2label=reverse_labels_dict\n",
        ")\n",
        "collator = ImageClassificationCollator(feature_extractor)\n",
        "vit_train_loader = DataLoader(vit_train_ds, batch_size=64, collate_fn=collator, num_workers=2, shuffle=True)\n",
        "vit_val_loader = DataLoader(vit_val_ds, batch_size=64, collate_fn=collator, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPGRz0tzzvam"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(vit_model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9r4pcwzz5yN"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 50\n",
        "num_training_steps = num_epochs * len(vit_train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i14caeWX0LYe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "vit_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukTy1Cni1h-g"
      },
      "outputs": [],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHsvzT9U0iPN"
      },
      "outputs": [],
      "source": [
        "# from tqdm.auto import tqdm\n",
        "\n",
        "# progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "# vit_model.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     total_loss = 0\n",
        "#     total_acc = 0\n",
        "#     count = 0\n",
        "#     for batch in vit_train_loader:\n",
        "#         count += 1\n",
        "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#         outputs = vit_model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         logits = outputs.logits\n",
        "#         predictions = torch.argmax(logits, dim=-1)\n",
        "#         total_acc += (predictions == batch['labels']).sum()/len(batch)\n",
        "#         print(predictions, batch['labels'])\n",
        "#         optimizer.step()\n",
        "#         lr_scheduler.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         progress_bar.update(1)\n",
        "#     print(\"Loss: \" + str(total_loss/count) + \" Accuracy: \" + str(total_acc.item()/count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIXw7UHreHoM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "TpK-U9c5NxQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip install transformers==4.28.0"
      ],
      "metadata": {
        "id": "yblgm58vZTgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXunkLFSd0Vq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-model_basic\",\n",
        "  per_device_train_batch_size=64,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=50,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-2,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ST0S8XHf0kP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bY4K3JBfkRN"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=vit_model,\n",
        "    args=training_args,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=vit_train_ds,\n",
        "    eval_dataset=vit_val_ds,\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM1zI_35hpKA"
      },
      "outputs": [],
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62o9gQcwhuUc"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(vit_val_ds)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMkIBzZR1YK_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVS7fxQ-1m_-"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "vit_model.eval()\n",
        "for batch in vit_val_loader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = vit_model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}