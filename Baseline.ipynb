{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2i0h6Uxz0bg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import yaml\n",
        "import sys\n",
        "import os\n",
        "from IPython import display\n",
        "import time\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from tqdm import tqdm\n",
        "# from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1bGY5UP0EK9",
        "outputId": "f75f8476-f320-496a-847a-607aafc3bb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "\n",
        "  import torchvision\n",
        "  print(\"PyTorch version:\", torch.__version__)\n",
        "  print(\"Torchvision version:\", torchvision.__version__)\n",
        "  print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "  import sys\n",
        "  !{sys.executable} -m pip install opencv-python matplotlib\n",
        "  !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "  from segment_anything.utils.amg import *\n",
        "  from pycocotools import mask as mask_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FMCmO3I0Nbj",
        "outputId": "379de041-2942-4720-9a25-3fcf83f0d451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "Torchvision version: 0.15.2+cu118\n",
            "CUDA is available: True\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-ey53hf0a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-ey53hf0a\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "      'Metal (general)',\n",
        "      'Metal (iron bender)',\n",
        "      'Metal (pipe)',\n",
        "      'Wood (pallett)',\n",
        "      'Wood (scraps/cuttings)',\n",
        "      'Carton',\n",
        "      'Concrete',\n",
        "      'Plastic (big bag)',\n",
        "      'Plastic (bucket)',\n",
        "      'Plastic (general)',\n",
        "      'Plastic (pipe)',\n",
        "      'Plastic (sand bag)',\n",
        "      'Nylon',\n",
        "      'Rubber',\n",
        "      'Textil/Fabric/Cloth',\n",
        "      'Background',\n",
        "      'All Background',\n",
        "      'Unknown',\n",
        "      'Skip',\n",
        "      'Back',\n",
        "      'Exit'\n",
        "  ]"
      ],
      "metadata": {
        "id": "7SjguJuz0U1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "    polygons = []\n",
        "    color = []\n",
        "    for ann in sorted_anns:\n",
        "        m = rle_to_mask(ann['segmentation'])\n",
        "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
        "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "        for i in range(3):\n",
        "            img[:,:,i] = color_mask[i]\n",
        "        ax.imshow(np.dstack((img, m*0.35)))"
      ],
      "metadata": {
        "id": "cydj7A8b0brn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_box_from_mask(mask):\n",
        "  mask = rle_to_mask(mask)\n",
        "  pixel_list = np.argwhere(mask)\n",
        "  min_x = min(pixel[1] for pixel in pixel_list)\n",
        "  min_y = min(pixel[0] for pixel in pixel_list)\n",
        "  max_x = max(pixel[1] for pixel in pixel_list)\n",
        "  max_y = max(pixel[0] for pixel in pixel_list)\n",
        "  box = [min_x, min_y, max_x, max_y]\n",
        "  return box"
      ],
      "metadata": {
        "id": "OkgL4yo40e7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def crop_object(image, mask):\n",
        "#     cropped = []\n",
        "#     for i in range(mask.shape[0]):\n",
        "#         img = image[i,:,:,:].squeeze()\n",
        "#         m = mask[i,:,:].squeeze()\n",
        "#         contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "#         largest_contour = max(contours, key=cv2.contourArea)\n",
        "#         contour_mask = np.zeros_like(m, dtype=np.uint8)\n",
        "#         cv2.drawContours(contour_mask, [largest_contour], 0, 255, -1)\n",
        "#         cropped_image = cv2.bitwise_and(img, img, mask=contour_mask)\n",
        "#         cropped.append(cropped_image)\n",
        "#     return np.array(cropped)"
      ],
      "metadata": {
        "id": "rP25UEl4Vqsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from skimage import transform, img_as_ubyte\n",
        "\n",
        "class base(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(base, self).__init__()\n",
        "    self.fc1 = nn.Linear(3, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 9)\n",
        "    self.seq = nn.Sequential(self.fc1, nn.ReLU(), self.fc2, nn.ReLU(), self.fc3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    image, mask = x\n",
        "    masked = (mask.unsqueeze(3) * image).cuda()\n",
        "    col = masked.sum(axis=[1, 2]) / mask.sum()\n",
        "    x = self.seq(col)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xLzaOIaU0iVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Project/Project/data/Arik\""
      ],
      "metadata": {
        "id": "w6yESNUl5b8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = 'DJI_20230221091035_0034_V'\n",
        "split = 0"
      ],
      "metadata": {
        "id": "ELUGV3_o5kwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {'Metal (general)':0, 'Metal (iron bender)':0, 'Metal (pipe)':0,\n",
        "               'Wood (pallett)':1, 'Wood (scraps/cuttings)':1,\n",
        "               'Carton':2,\n",
        "               'Concrete':3,\n",
        "               'Plastic (big bag)':4, 'Plastic (bucket)':4, 'Plastic (general)':4,\n",
        "               'Plastic (pipe)':4, 'Plastic (sand bag)':4,\n",
        "               'Nylon':5,\n",
        "               'Rubber':6,\n",
        "               'Textil/Fabric/Cloth':7,\n",
        "               'Background':8,\n",
        "               'Unknown':9}"
      ],
      "metadata": {
        "id": "iEm9FiCi9-IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class SplitDataset(Dataset):\n",
        "  def __init__(self, folder_path, image_name, sp):\n",
        "    self.image_name = image_name\n",
        "    self.sp = sp\n",
        "    self.im_path = f'{folder_path}/{image_name}'\n",
        "    self.split_path = f'{self.im_path}/split_{sp}'\n",
        "    with open(f'{self.split_path}/seg_{sp}.yaml') as f:\n",
        "      self.seg = yaml.safe_load(f)\n",
        "    files_in_folder = os.listdir(self.split_path)\n",
        "    self.ann_dict={image_name:{sp:{}}}\n",
        "    if f'anns_{sp}.yaml' in files_in_folder:\n",
        "      with open(f'{self.split_path}/anns_{sp}.yaml', 'r') as f:\n",
        "        self.ann_dict = yaml.safe_load(f)\n",
        "        # Remove all unknowns\n",
        "        self.ann_dict[self.image_name][self.sp] = {key:val for key, val in self.ann_dict[self.image_name][self.sp].items() if val != 'Unknown'}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ann_dict[self.image_name][self.sp])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    annotated_masks = list(self.ann_dict[self.image_name][self.sp].keys())\n",
        "    m = annotated_masks[idx]\n",
        "    img = cv2.imread(f'{self.split_path}/split_{self.sp}.jpg')\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    mask = rle_to_mask(self.seg[self.image_name][self.sp][m]['segmentation'])\n",
        "    label = self.ann_dict[self.image_name][self.sp][m]\n",
        "    y = labels_dict[label]\n",
        "    return (img, mask), y"
      ],
      "metadata": {
        "id": "hTC18nee_b-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = []\n",
        "for split in range(16):\n",
        "  ds = SplitDataset(folder_path, image, split)\n",
        "  arr.append(ds)\n",
        "\n",
        "curr_dataset = torch.utils.data.ConcatDataset(arr)"
      ],
      "metadata": {
        "id": "Ljepw-x0HSjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(curr_dataset[0])"
      ],
      "metadata": {
        "id": "6qMYGCHMWdoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# curr_dataset = []\n",
        "# main_data_folder = \"/content/drive/MyDrive/Project/Project/data\"\n",
        "# for name in os.listdir(main_data_folder):\n",
        "#   for image in os.listdir(f'{main_data_folder}/{name}'):\n",
        "#     for split in range(16):\n",
        "#       ds = SplitDataset(f'{main_data_folder}/{name}', image, split)\n",
        "#       curr_dataset = torch.utils.data.ConcatDataset([curr_dataset, ds])"
      ],
      "metadata": {
        "id": "8cHm3v0ADddS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = torch.utils.data.random_split(curr_dataset, [0.6, 0.2, 0.2])\n",
        "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "valid_dataloader = DataLoader(val_ds, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=32, shuffle=True)\n",
        ""
      ],
      "metadata": {
        "id": "cQa5hh2gF0Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "model = base().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = torch.nn.CrossEntropyLoss().cuda()"
      ],
      "metadata": {
        "id": "wvmbLh2FLhsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    running_accuracy = 0.\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs).cuda()\n",
        "        # outputs = model(inputs)\n",
        "        running_accuracy += (labels == outputs.argmax(dim=1)).float().mean()\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:\n",
        "            last_loss = running_loss / 10 # loss per batch\n",
        "            last_accuracy = running_accuracy / 10\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            print('  batch {} accuracy: {}'.format(i + 1, last_accuracy))\n",
        "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss, last_accuracy"
      ],
      "metadata": {
        "id": "4UKFYYh85nMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss, avg_accuracy = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    running_vaccuracy = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(valid_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "            vlabels = vlabels.cuda()\n",
        "            voutputs = model(vinputs).cuda()\n",
        "            # voutputs = model(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vaccuracy += (vlabels == voutputs.argmax(dim=1)).float().mean()\n",
        "            running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    avg_vaccuracy = running_vaccuracy / (i + 1)\n",
        "\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "    print('ACCURACY train {} valid {}'.format(avg_accuracy, avg_vaccuracy))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "o9HcX4IfMnBR",
        "outputId": "92a847f2-9944-4e6c-e9f7-01a15f3a4594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1ed2d86884ea>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Make sure gradient tracking is on, and do a pass over the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-1f3c54b2b017>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Every data instance is an input + label pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Zero your gradients for every batch!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}